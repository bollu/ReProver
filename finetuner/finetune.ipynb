{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4606fcad-8d0d-48bb-af83-8e54ea45148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru in /anaconda/envs/py38_default/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: transformers in /anaconda/envs/py38_default/lib/python3.8/site-packages (4.31.0)\n",
      "Requirement already satisfied: wandb in /anaconda/envs/py38_default/lib/python3.8/site-packages (0.15.8)\n",
      "Requirement already satisfied: accelerate in /anaconda/envs/py38_default/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: datasets in /anaconda/envs/py38_default/lib/python3.8/site-packages (2.14.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (1.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: requests in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (1.29.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (65.5.0)\n",
      "Requirement already satisfied: pathtools in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: setproctitle in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from wandb) (3.1.32)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (1.13.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: multiprocess in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: xxhash in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: pandas in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: six>=1.4.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru transformers wandb transformers[torch] accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "40ca3e26-17ac-4f84-997b-28ad5105cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Fine tune CodeT5 model on the FStar everest dataset.\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import datetime\n",
    "from typing import *\n",
    "from loguru import logger\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from transformers import (\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    BertConfig, BertForMaskedLM, BertTokenizer,\n",
    "    GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
    "    OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
    "    RobertaConfig, RobertaModel, RobertaTokenizer,\n",
    "    DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer,\n",
    ")\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Seq2SeqTrainer,AutoTokenizer, T5ForConditionalGeneration,EarlyStoppingCallback, Seq2SeqTrainingArguments, AdamW, ProgressCallback\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "# model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# https://huggingface.co/transformers/v3.0.2/model_doc/t5.html#t5forconditionalgeneration\n",
    "# >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# \n",
    "# >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "# >>> model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "# >>> input_ids = tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1\n",
    "# >>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)\n",
    "# >>> loss, prediction_scores = outputs[:2]\n",
    "# \n",
    "# >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "# >>> model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "# >>> input_ids = tokenizer.encode(\"summarize: Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1\n",
    "# >>> outputs = model.generate(input_ids)\n",
    "#\n",
    "\n",
    "### Tutorial: https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "640da8a6-8bce-4e7b-ace9-762d9aacd31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='Salesforce/codet5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "max_model_length = tokenizer.model_max_length #for CodeT5 it is 512\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fb0e9134-c59b-48ee-92d9-bb3bde92d03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-08-04 22:20:04.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mmodel EOS token: </s>\u001b[0m\n",
      "\u001b[32m2023-08-04 22:20:04.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mspecial tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>', '<extra_id_99>', '<extra_id_98>', '<extra_id_97>', '<extra_id_96>', '<extra_id_95>', '<extra_id_94>', '<extra_id_93>', '<extra_id_92>', '<extra_id_91>', '<extra_id_90>', '<extra_id_89>', '<extra_id_88>', '<extra_id_87>', '<extra_id_86>', '<extra_id_85>', '<extra_id_84>', '<extra_id_83>', '<extra_id_82>', '<extra_id_81>', '<extra_id_80>', '<extra_id_79>', '<extra_id_78>', '<extra_id_77>', '<extra_id_76>', '<extra_id_75>', '<extra_id_74>', '<extra_id_73>', '<extra_id_72>', '<extra_id_71>', '<extra_id_70>', '<extra_id_69>', '<extra_id_68>', '<extra_id_67>', '<extra_id_66>', '<extra_id_65>', '<extra_id_64>', '<extra_id_63>', '<extra_id_62>', '<extra_id_61>', '<extra_id_60>', '<extra_id_59>', '<extra_id_58>', '<extra_id_57>', '<extra_id_56>', '<extra_id_55>', '<extra_id_54>', '<extra_id_53>', '<extra_id_52>', '<extra_id_51>', '<extra_id_50>', '<extra_id_49>', '<extra_id_48>', '<extra_id_47>', '<extra_id_46>', '<extra_id_45>', '<extra_id_44>', '<extra_id_43>', '<extra_id_42>', '<extra_id_41>', '<extra_id_40>', '<extra_id_39>', '<extra_id_38>', '<extra_id_37>', '<extra_id_36>', '<extra_id_35>', '<extra_id_34>', '<extra_id_33>', '<extra_id_32>', '<extra_id_31>', '<extra_id_30>', '<extra_id_29>', '<extra_id_28>', '<extra_id_27>', '<extra_id_26>', '<extra_id_25>', '<extra_id_24>', '<extra_id_23>', '<extra_id_22>', '<extra_id_21>', '<extra_id_20>', '<extra_id_19>', '<extra_id_18>', '<extra_id_17>', '<extra_id_16>', '<extra_id_15>', '<extra_id_14>', '<extra_id_13>', '<extra_id_12>', '<extra_id_11>', '<extra_id_10>', '<extra_id_9>', '<extra_id_8>', '<extra_id_7>', '<extra_id_6>', '<extra_id_5>', '<extra_id_4>', '<extra_id_3>', '<extra_id_2>', '<extra_id_1>', '<extra_id_0>']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "eos_token = tokenizer.eos_token\n",
    "logger.info(f\"model EOS token: {eos_token}\")\n",
    "logger.info(f\"special tokens: {tokenizer.all_special_tokens}\")\n",
    "# #updating the tokenizer's vocalblary file with End of Statement <EOS> Special Token:\n",
    "# # print(\"Tokenizer's original size:  \",len(tokenizer))\n",
    "# special_tokens_dict = {'eos_token': '<EOS>'}\n",
    "# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# # print('\\n We have added', num_added_toks, 'token')\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# # print(tokenizer.all_special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "94bed5b8-06a4-4d19-aa93-d6ca3f5f84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aa5ce053-29b8-471c-814f-6f109ee0d4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-08-04 22:20:07.399\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[34m\u001b[1meos token: </s> | encoded {'input_ids': [1, 2, 2], 'attention_mask': [1, 1, 1]}\u001b[0m\n",
      "\u001b[32m2023-08-04 22:20:07.401\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mtokenizer tokenize: ['foo', 'Ġbar', 'Ġbaz', 'Ġqu', 'ux', 'l', 'aj', 'das', 'ds', 'ad', 'l', 'ka']\u001b[0m\n",
      "\u001b[32m2023-08-04 22:20:07.402\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mtokenizer funcall: {'input_ids': [1, 11351, 4653, 29025, 719, 2616, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[32m2023-08-04 22:20:07.402\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[34m\u001b[1mconvert_tokens_to_ids: [11351, 4653, 29025, 719, 2616, 5604, 84]\u001b[0m\n",
      "\u001b[32m2023-08-04 22:20:07.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1meos token: </s> | encoded {'input_ids': [1, 2, 2], 'attention_mask': [1, 1, 1]}\u001b[0m\n",
      "\u001b[32m2023-08-04 22:20:07.404\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m44\u001b[0m - \u001b[34m\u001b[1mtokenizer tokenize: ['foo', 'Ġbar', 'Ġbaz', 'Ġqu', 'ux', 'l', 'aj', 'das', 'ds', 'ad', 'l', 'ka']\u001b[0m\n",
      "\u001b[32m2023-08-04 22:20:07.405\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m45\u001b[0m - \u001b[34m\u001b[1mtokenizer funcall: {'input_ids': [1, 11351, 4653, 29025, 719, 2616, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[32m2023-08-04 22:20:07.406\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m46\u001b[0m - \u001b[34m\u001b[1mconvert_tokens_to_ids: [11351, 4653, 29025, 719, 2616, 5604, 84]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def get_label_ids_sakina(target):\n",
    "    \"\"\"\n",
    "    Why is it correct for the model to produce <EOS> at the end if we have too large a sentence?\n",
    "    Siddharth does not believe this implementation.\n",
    "    \"\"\"\n",
    "    max_length=tokenizer.model_max_length\n",
    "    # to train model on End of statement token. Even When model truncates longer code, EOS remain to show model the end of the statement\n",
    "    # Tokenize the target text without padding to get the tokens\n",
    "    encoded_tokens = tokenizer.tokenize(target)\n",
    "    # Check if the total number of tokens is greater than max_length\n",
    "    if len(encoded_tokens) > max_length:\n",
    "        # If yes, truncate the tokens while preserving the \"<EOS>\" at the end\n",
    "        truncated_tokens = encoded_tokens[:max_length - 1] + [encoded_tokens[-1]]\n",
    "        # Convert the truncated tokens back to input_ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(truncated_tokens)\n",
    "    else:\n",
    "        # If no truncation needed, keep the original tokens with padding\n",
    "        input_ids = tokenizer(target, max_length=max_length, padding=\"max_length\", truncation=True).input_ids\n",
    "    # print(input_ids)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def get_label_ids(target):\n",
    "    \"\"\"\n",
    "    get  {'input_ids': [1, 11351, 4653, 29025, 719, 2616, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
    "    for input \"foo bar baz quux larp\"\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(target, max_length=max_length, padding=\"max_length\", truncation=True).input_ids\n",
    "    return input_ids\n",
    "\n",
    "if True: # testing\n",
    "    eos_encoded = tokenizer(tokenizer.eos_token)\n",
    "    logger.debug(f'eos token: {tokenizer.eos_token} | encoded {eos_encoded}')\n",
    "    logger.debug(f'tokenizer tokenize: {tokenizer.tokenize(\"foo bar baz quuxlajdasdsadlka\")}')\n",
    "    logger.debug(f'tokenizer funcall: {tokenizer(\"foo bar baz quux\")}')\n",
    "    logger.debug(f'convert_tokens_to_ids: {tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"foo bar baz quux larp\"))}')\n",
    "# tokenize: string -> List[token=str]\n",
    "# convert_tokens_to_ids: List[token=str] -> List[int]\n",
    "# tokenizer(...) = convert_tokns_to_ids . tokenize + attention mask.\n",
    "# \n",
    "if True: # testing\n",
    "    eos_encoded = tokenizer(tokenizer.eos_token)\n",
    "    logger.debug(f'eos token: {tokenizer.eos_token} | encoded {eos_encoded}')\n",
    "    logger.debug(f'tokenizer tokenize: {tokenizer.tokenize(\"foo bar baz quuxlajdasdsadlka\")}')\n",
    "    logger.debug(f'tokenizer funcall: {tokenizer(\"foo bar baz quux\")}')\n",
    "    logger.debug(f'convert_tokens_to_ids: {tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"foo bar baz quux larp\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b5de7be8-ccfb-4b18-8a73-9e9dd902a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maping the dataset into batches\n",
    "Experiment = True # for the first experimental run to get the pipeline going\n",
    "if Experiment and not os.path.exists(\"input.txt\"):\n",
    "    # Download Shakespeare\n",
    "    %time\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "66626f42-d850-4996-b782-8c21a0785ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 2699/2699 [00:32<00:00, 83.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'val finv: res:felem -> a:felem -> Stack unit\\n\\n  (requires fun h ->\\n\\n    live h a /\\\\ live h res /\\\\ eq_or_disjoint a res /\\\\\\n\\n    as_nat h a < S.prime)\\n\\n  (ensures fun h0 _ h1 -> modifies (loc res) h0 h1 /\\\\\\n\\n    as_nat h1 res < S.prime /\\\\\\n</s>'}, {'input': 'val fsqrt: res:felem -> a:felem -> Stack unit\\n\\n  (requires fun h ->\\n\\n    live h a /\\\\ live h res /\\\\ eq_or_disjoint a res /\\\\\\n\\n    as_nat h a < S.prime)\\n\\n  (ensures fun h0 _ h1 -> modifies (loc res) h0 h1 /\\\\\\n\\n    as_nat h1 res < S.prime /\\\\\\n</s>'}, {'input': \"type t19' =\\n\\n  | X_a of (squash False)\\n</s>\"}]\n",
      "#defs: 49728\n",
      "grabbed output from #files: 2647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import tqdm\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "defs = []\n",
    "\n",
    "files = set()\n",
    "for jpath in tqdm.tqdm(glob.glob(\"dataset/*.json\")):\n",
    "    j = json.loads(open(jpath, \"r\").read())\n",
    "    for jdefn in j[\"defs\"]:\n",
    "        filepath = pathlib.Path(jdefn[\"file_name\"]).name \n",
    "        files.add(filepath)\n",
    "        data = open(f\"./raw_dataset/{filepath}\").readlines()\n",
    "        start_line = int(jdefn[\"start_line\"])\n",
    "        end_line = int(jdefn[\"end_line\"])\n",
    "        if start_line == 0: continue # start line is zero.\n",
    "        data = \"\\n\".join(data[start_line-1:end_line-1])\n",
    "        if data:\n",
    "            defs.append({\"input\": data + tokenizer.eos_token})\n",
    "        \n",
    "print(defs[:3])\n",
    "print(f\"#defs: {len(defs)}\")\n",
    "files = sorted(list(files))\n",
    "print(f\"grabbed output from #files: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "373f260a-51d5-484b-ae65-559adaf00857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25bfc8d61144a9289f16ec2e77615c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (985 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374e3bbac4cd456c8b1060cc0bad3eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe16ebe74c8f4e1e8ed35fb71bae21b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da14a7286d040bdb3fab3a990089654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9488cfb6e604ebc8adfae3f43dc7136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0d18e863e64d2d938357b898145399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://github.com/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb\n",
    "### Tutorial: https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling\n",
    "def build_huggingface_dataset_from_list_of_defs(defs: List[Dict[str, Any]]) -> datasets.Dataset:\n",
    "    dataset = datasets.Dataset.from_list(defs)\n",
    "    dataset = dataset.map(lambda egs : tokenizer(egs[\"input\"]), batched=True, num_proc=4)\n",
    "    block_size = 128\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        # concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        concatenated_examples = examples\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, though you could add padding instead if the model supports it\n",
    "        # In this, as in all things, we advise you to follow your heart\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "    dataset = dataset.map(group_texts, batched=True, batch_size=1000, num_proc=4)\n",
    "    return dataset \n",
    "\n",
    "defs = defs[:1000]\n",
    "train = build_huggingface_dataset_from_list_of_defs(defs[:int(len(defs)*0.8)])\n",
    "valid = build_huggingface_dataset_from_list_of_defs(lines[int(len(defs)*0.8):int(len(defs)*0.9)])\n",
    "test = build_huggingface_dataset_from_list_of_defs(lines[int(len(defs)*0.9):])\n",
    "# logger.info(f\"len train: {len(train)} | test: {len(test)} | valid: {len(valid)}\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fa5117b9-d5a5-4fa0-852e-99469c7133b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_eg: dict_keys(['input', 'input_ids', 'attention_mask', 'labels'])\n",
      "train_eg input: <class 'list'> | len : 128\n",
      "input[0]: val finv: res:felem -> a:felem -> Stack unit\n",
      "\n",
      "  (requires fun h ->\n",
      "\n",
      "    live h a /\\ live h\n"
     ]
    }
   ],
   "source": [
    "def debug():\n",
    "    train_eg = next(iter(train))\n",
    "    print(f\"train_eg: {train_eg.keys()}\")\n",
    "    train_eg_input = train_eg['input']\n",
    "    print(f\"train_eg input: {type(train_eg_input)} | len : {len(train_eg_input)}\")\n",
    "    print(f\"input[0]: {train_eg_input[0][:90]}\")\n",
    "debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0e8795d3-d605-451a-804f-a5b204761b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_dir\",\n",
    "    learning_rate=2e-5, # should I use a much smaller learning rate?\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10\n",
    ")\n",
    "training_args = training_args.set_dataloader(train_batch_size=512, eval_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9e3c5bc9-383c-4706-9b83-9bb613bc10a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:18791fi3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51773d4fefe046e390e182da4ba6a68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▆▁█</td></tr><tr><td>eval/runtime</td><td>▁▃█</td></tr><tr><td>eval/samples_per_second</td><td>█▆▁</td></tr><tr><td>eval/steps_per_second</td><td>█▇▁</td></tr><tr><td>train/epoch</td><td>▁▅██</td></tr><tr><td>train/global_step</td><td>▁▅██</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>28.0529</td></tr><tr><td>eval/runtime</td><td>3.0366</td></tr><tr><td>eval/samples_per_second</td><td>164.656</td></tr><tr><td>eval/steps_per_second</td><td>0.329</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>24</td></tr><tr><td>train/total_flos</td><td>78354444189696.0</td></tr><tr><td>train/train_loss</td><td>27.39769</td></tr><tr><td>train/train_runtime</td><td>388.8744</td></tr><tr><td>train/train_samples_per_second</td><td>30.858</td></tr><tr><td>train/train_steps_per_second</td><td>0.062</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-capybara-10</strong> at: <a href='https://wandb.ai/microsoft-research-incubation/codet5-finetune-fstar/runs/18791fi3' target=\"_blank\">https://wandb.ai/microsoft-research-incubation/codet5-finetune-fstar/runs/18791fi3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230804_100527-18791fi3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:18791fi3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23b35a9898c4c4c813914ccf809e403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670058597810566, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/t-sibhat/neural-premise-selection/src/Projects/premise-selection/ReProver/finetuner/wandb/run-20230804_222108-ux4qm7pw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/microsoft-research-incubation/codet5-finetune-fstar/runs/ux4qm7pw' target=\"_blank\">skilled-shadow-11</a></strong> to <a href='https://wandb.ai/microsoft-research-incubation/codet5-finetune-fstar' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/microsoft-research-incubation/codet5-finetune-fstar' target=\"_blank\">https://wandb.ai/microsoft-research-incubation/codet5-finetune-fstar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/microsoft-research-incubation/codet5-finetune-fstar/runs/ux4qm7pw' target=\"_blank\">https://wandb.ai/microsoft-research-incubation/codet5-finetune-fstar/runs/ux4qm7pw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Trainer is attempting to log a value of \"{'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}\" for key \"task_specific_params\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 22:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>26.996200</td>\n",
       "      <td>27.893749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>26.859500</td>\n",
       "      <td>27.339882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>26.049900</td>\n",
       "      <td>26.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>25.352500</td>\n",
       "      <td>25.531055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>24.825900</td>\n",
       "      <td>25.317810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>24.560700</td>\n",
       "      <td>24.696352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>24.270400</td>\n",
       "      <td>24.838930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>23.898100</td>\n",
       "      <td>24.354298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from transformers import Trainer\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"codet5-finetune-fstar\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={})\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "28aa674f-cdfa-4d26-bd33-2ddcd6441d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time: '1385 days, 13:45:38.095093' \n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "calc_time = \"Total Training Time: '%s' \" % datetime.timedelta(elapsed_time_secs)\n",
    "print(calc_time)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "80a84730-9540-4b04-a86d-0559587a284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./output_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "20a40abd-8f79-4b97-bb6a-b0b4531b9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h a / \\\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model_gen = T5ForConditionalGeneration.from_pretrained(\"./output_dir\")\n",
    "# model_gen = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\")\n",
    "outs_ids = model_gen.generate(tokenizer(\"live h a /\\ \", return_tensors=\"pt\").input_ids)\n",
    "out_str = tokenizer.decode(outs_ids[0], skip_special_tokens=True)\n",
    "print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbcd97-cb11-443e-b599-57e123e5c439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
